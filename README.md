# ğŸš€ Projeto: RedundÃ¢ncia de Arquivos com Azure Data Factory

## ğŸ¯ Objetivo do Projeto
Criar um pipeline automatizado no Azure Data Factory (ADF) para mover dados de um banco de dados SQL on-premises para um Azure Data Lake, salvando as informaÃ§Ãµes como arquivos .TXT em camadas de armazenamento (como raw/bronze).

## Este processo inclui:
âœ… ConfiguraÃ§Ã£o do Integration Runtime para conectar ao ambiente on-premises
âœ… CriaÃ§Ã£o de Linked Services e Datasets
âœ… ConstruÃ§Ã£o do Pipeline para mover os dados e convertÃª-los para .TXT
âœ… ValidaÃ§Ã£o, publicaÃ§Ã£o e execuÃ§Ã£o do pipeline
âœ… AnÃ¡lise de performance e boas prÃ¡ticas

## ğŸ—ï¸ Passo 1: ConfiguraÃ§Ã£o do Ambiente no Azure
Antes de construir o pipeline, vocÃª precisa configurar os seguintes serviÃ§os no Microsoft Azure:

1.1 Criar um Storage Account para armazenar os arquivos
Acesse o Azure Portal (https://portal.azure.com)

Pesquise por Storage Accounts e clique em Criar

Escolha um Resource Group ou crie um novo

Defina um nome para o Storage Account (exemplo: meuarmazenamento)

Escolha RegiÃ£o, Desempenho Standard, ReplicaÃ§Ã£o LRS

Clique em Review + Create e depois em Create

1.2 Criar um Azure Data Lake Storage (ADLS)
No Storage Account criado, vÃ¡ para Containers

Clique em + Container e nomeie como bronze (para a camada raw)

Defina as permissÃµes como Private e clique em Criar

1.3 Criar um Banco de Dados SQL no Azure (se necessÃ¡rio)
No Azure Portal, vÃ¡ para SQL Databases > Criar Banco de Dados

Escolha um nome (exemplo: meudb) e associe a um SQL Server existente ou crie um novo

Defina a camada de serviÃ§o (por exemplo, Basic)

Clique em Review + Create e depois em Create

Acesse o banco pelo SQL Server Management Studio (SSMS) e crie a tabela de exemplo:

```sql
Copiar
Editar
CREATE TABLE Clientes (
    ID INT PRIMARY KEY,
    Nome VARCHAR(100),
    Email VARCHAR(100),
    DataCriacao DATETIME DEFAULT GETDATE()
);
```

## ğŸ”— Passo 2: Configurar o Azure Data Factory
2.1 Criar um Azure Data Factory
No Azure Portal, pesquise por Data Factory e clique em Criar

Escolha um nome (exemplo: meuADF)

Selecione um Resource Group e regiÃ£o

Clique em Review + Create e depois em Create

2.2 Criar um Integration Runtime para conectar ao SQL On-Premises
Se seu banco de dados estÃ¡ on-premises, vocÃª precisa configurar um Self-Hosted Integration Runtime:

No ADF, vÃ¡ para Manage > Integration Runtimes

Clique em + New > Escolha Self-Hosted

Instale o runtime no servidor on-premises e conecte ao Azure

## ğŸ”„ Passo 3: Criar os Linked Services
Linked Services sÃ£o as conexÃµes entre o ADF e os serviÃ§os externos.

3.1 Criar um Linked Service para o SQL Server On-Premises
No ADF, vÃ¡ para Manage > Linked Services

Clique em + New e escolha Azure SQL Database ou SQL Server

Se for on-premises, selecione o Integration Runtime criado

Insira os detalhes do banco (Server Name, Database Name, User, Password)

Teste a conexÃ£o e clique em Create

3.2 Criar um Linked Service para o Azure Data Lake
VÃ¡ em Linked Services > + New

Escolha Azure Data Lake Storage Gen2

Insira os detalhes do Storage Account criado

Teste a conexÃ£o e clique em Create

## ğŸ“‚ Passo 4: Criar os Datasets
Datasets representam as tabelas (origem) e os arquivos (destino).

4.1 Criar um Dataset para o SQL Server
No ADF, vÃ¡ para Author > Datasets > + New Dataset

Escolha Azure SQL Database (ou SQL Server)

Selecione o Linked Service do SQL criado

Escolha a tabela Clientes e clique em Create

4.2 Criar um Dataset para o Azure Data Lake (arquivos .TXT)
Em Datasets, clique em + New Dataset

Escolha Azure Data Lake Storage Gen2

Escolha o formato DelimitedText (CSV/TXT)

Selecione o Linked Service do ADLS

Configure o caminho do container (/bronze/)

Marque First row as header e clique em Create

## ğŸ”€ Passo 5: Criar o Pipeline no Azure Data Factory
5.1 Criar um Pipeline de CÃ³pia de Dados
No ADF, vÃ¡ para Author > Pipelines > + New Pipeline

Arraste a atividade Copy Data para o canvas

Configure:

Source: Escolha o Dataset do SQL Server

Sink: Escolha o Dataset do Azure Data Lake

File Name: Defina um nome dinÃ¢mico, como @concat('Clientes_', formatDateTime(utcNow(), 'yyyyMMddHHmmss'), '.txt')

Clique em Debug para testar

5.2 Publicar e Executar o Pipeline
Clique em Publish All para salvar as configuraÃ§Ãµes

VÃ¡ para Trigger > New/Now para rodar manualmente

## ğŸ“Š Passo 6: Monitorar e Analisar a Performance
VÃ¡ para Monitor no ADF

Veja a execuÃ§Ã£o do pipeline e cheque logs de erro (se houver)

Ajuste a Performance configurando Batch Size e Parallelism

## ğŸš€ ConclusÃ£o
VocÃª configurou um pipeline de redundÃ¢ncia de arquivos no Azure, movendo dados do SQL Server on-premises para um Azure Data Lake, convertendo-os em arquivos .TXT organizados na camada raw/bronze.

ğŸ“¢ PrÃ³ximos Passos:
ğŸ”¹ Melhorar a performance usando Data Flow
ğŸ”¹ Criar uma automaÃ§Ã£o com triggers (ex: execuÃ§Ã£o diÃ¡ria)
ğŸ”¹ Implementar seguranÃ§a e auditoria
